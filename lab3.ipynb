{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0c5NKeibz9m4",
        "4DRvswhK6nnj",
        "v-iyfoco-67K",
        "Q9bjZx00MZgv",
        "FthTQXQzFf_l",
        "6pNMAWpEnm9i",
        "avQvEEuzo5oD",
        "c1mRxcflqQHD",
        "dRNMEj-RxPsZ",
        "QQIl9AP60Ooj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUWm0aQb9BC8"
      },
      "source": [
        "---\n",
        "# <font color=\"#CA3532\">Deep Learning Fundamentals and Basic Tools - Lab Assignment 3</font>\n",
        "---\n",
        "\n",
        "Last updated on 2020-10-09\n",
        "\n",
        "Please report any bugs to luis.lago@uam.es\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ypcZkZYAigr"
      },
      "source": [
        "This lab is divided in two sessions:\n",
        "\n",
        "- *SESSION 1 (Friday, 2020-10-09):* Exercises included in this notebook.\n",
        "\n",
        "- *SESSION 2 (Friday, 2020-10-16):* Open project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FgFUcw19sIG"
      },
      "source": [
        "### <font color=\"#CA3532\">Instructions</font>\n",
        "\n",
        "- The assignment must be done in pairs.\n",
        "\n",
        "- All the exercises for the first session are included in this notebook. Complete the sections in the code marked with ``TO-DO`` comments and upload the notebook to the course [Moodle page](https://posgrado.uam.es/course/view.php?id=41889) before the due date. \n",
        "\n",
        "- No code should be included out of the ``TO-DO`` blocks.\n",
        "\n",
        "- Do not forget to include your names and NIAs in the following code block. \n",
        "\n",
        "- **Due date (SESSION 1 exercises):** Thursday, 2020-10-22, 23:59.\n",
        "\n",
        "- Specific instructions for SESSION 2 may be found at the end of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMNCfXr8E9Y5"
      },
      "source": [
        "# TO-DO: Include your names and NIAs here:\n",
        "student_data = [{'name': 'Name of 1st student', 'nia': 'NIA of 1st student'},\n",
        "                {'name': 'Name of 2nd student', 'nia': 'NIA of 2nd student'}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c5NKeibz9m4"
      },
      "source": [
        "### <font color=\"#CA3532\">Work environment</font>\n",
        "\n",
        "All the code for this lab has been developed with *Google Colab*, a tool by Google, based on the [*Jupyter Notebook*](https://jupyter.org/), that allows to execute Python code in a browser, with almost no configuration required. You can find more information in the following links:\n",
        "\n",
        "- https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "\n",
        "- https://www.youtube.com/watch?v=inN8seMm7UI\n",
        "\n",
        "You will need a Google account. If you have never used *Colab* or *Jupyter* before, read this [Overview guide](https://colab.research.google.com/notebooks/basic_features_overview.ipynb).\n",
        "\n",
        "If you prefer to work in your local machine you will need to install *Python 3* (we suggest to use the [*Anaconda*](https://www.anaconda.com/) distribution), and [*TensorFlow*](https://www.tensorflow.org/).\n",
        "\n",
        "As an alternative you may use the lab computers, which have been with installed all the required software. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "areLUgwp4ZhC"
      },
      "source": [
        "### <font color=\"#CA3532\">Clone the lab's github repo</font>\n",
        "\n",
        "The easiest way of having all the lab material available from *Colab* is to clone our github repository:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUhhu4QTzbDn",
        "outputId": "5e919445-3b81-44bc-da23-d2c66f4b884d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/luisferuam/DLFBT-LAB"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'DLFBT-LAB' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zPXQteNbA9N"
      },
      "source": [
        "### <font color=\"#CA3532\">Import the libraries</font>\n",
        "\n",
        "The following code cells import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qoR3iD7bLKF"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "import pickle\n",
        "\n",
        "from time import time\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1qAknm_nF70"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZBohNN5nH7f"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ3mdiCYu8HZ"
      },
      "source": [
        "import sys\n",
        "sys.path.append('DLFBT-LAB')\n",
        "import dlfbt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWFKq-SHKBft"
      },
      "source": [
        "---\n",
        "# <font color=\"#CA3532\">Lab session 1</font>\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DRvswhK6nnj"
      },
      "source": [
        "### <font color=\"#CA3532\">Introduction to PyTorch</font>\n",
        "\n",
        "[PyTorch](https://pytorch.org/) is an open source machine learning framework that may be a good alternative to [TensorFlow](https://www.tensorflow.org/). It is usually the preferred option amongst the academic comunity because it offers more flexibility and hence it is easier to build custom models. However it lacks a high level layer like [Keras](https://keras.io/), so development could be a bit harder. In the following links you can find comparisons between the two frameworks:\n",
        "\n",
        "- https://deepsense.ai/keras-or-pytorch/\n",
        "\n",
        "- https://wrosinski.github.io/deep-learning-frameworks/\n",
        "\n",
        "In this first part of the lab you will be introduced to the PyTorch framework.\n",
        "\n",
        "Follow the introductory tutorial in the [PyTorch 60-Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) and then solve the exercises.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVGPz5tsD1iJ"
      },
      "source": [
        "### <font color=\"#CA3532\">Automatic differentiation with PyTorch</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQeDujFAEPGn"
      },
      "source": [
        "As you have seen in the tutorial, the flag ``requires_grad=True`` may be used when creating a tensor to indicate that we will compute gradients with respect to it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C86e-JdkCjgg",
        "outputId": "6bf69b1e-365f-47a1-e166-3d7b4ef28768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxt6-qn_E2cE"
      },
      "source": [
        "Now, whenever we perform an operation with this tensor, the new tensor that results from this operation includes a function to compute the gradient (``grad_fn``):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjQo0ZcnD_vY",
        "outputId": "13427b7b-c996-4dff-bd6e-5635388382df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "y = x + 2\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwVdJhpcTg5h"
      },
      "source": [
        "Another example follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLu0aHfdD_sM",
        "outputId": "cf10a991-d004-443a-ad59-4605c4692cbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "print(z)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>)\n",
            "tensor(27., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gtLvuM8FHNW"
      },
      "source": [
        "To compute the gradients we must run the *backward pass* on the output tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PxF2JSmD_pC"
      },
      "source": [
        "out.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBTIfPryFM7E"
      },
      "source": [
        "And then we can access the gradient using the ``grad`` field:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlGVD58CD_lQ",
        "outputId": "dd727879-6fd7-44e1-9bcf-117515cef94f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyJxs-ZKFxEL"
      },
      "source": [
        "A simpler example, with scalar variables $x$ and $y$, follows:\n",
        "\n",
        "$$\n",
        "z = 2x^{2}y\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4inFyscD_gv",
        "outputId": "c4535a88-7fc5-4d5d-baf7-ba45d540e876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "x = torch.tensor(2., requires_grad=True)\n",
        "print(x)\n",
        "y = torch.tensor(3., requires_grad=True)\n",
        "print(y)\n",
        "z = 2*x*x*y\n",
        "print(z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2., requires_grad=True)\n",
            "tensor(3., requires_grad=True)\n",
            "tensor(24., grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ueq663yyGSmW"
      },
      "source": [
        "Backward pass:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc6Ac7htGRHF"
      },
      "source": [
        "z.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPf4FYqiGa2u"
      },
      "source": [
        "Gradients:\n",
        "\n",
        "$$\n",
        "\\frac{dz}{dx} = 4xy\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{dz}{dy} = 2x^{2}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34BW29GdGREy",
        "outputId": "b2a8d5f4-9f3f-4305-e1e8-7213b338e8c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(x.grad)\n",
        "print(y.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(24.)\n",
            "tensor(8.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJsqBAU2G0yn"
      },
      "source": [
        "### <font color=\"#CA3532\">Exercise 1</font>\n",
        "\n",
        "Use PyTorch to find the minimum of the function $y = x^{2}$ starting from and initial guess $x_{0}$ and using gradient descent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpRBIS0KArNl",
        "outputId": "b07cad05-8b3f-419d-9468-4de3df32e3bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "# Random initial:\n",
        "x_numpy = np.random.randn()\n",
        "print(\"x0 = %.4f\" % (x_numpy))\n",
        "\n",
        "# Number of iterations and learning rate:\n",
        "niters = 50\n",
        "lr = 0.1\n",
        "\n",
        "# Optimization loop:\n",
        "hh = []\n",
        "for i in range(niters):\n",
        "  #-----------------------------------------------------------------------------\n",
        "  # TO-DO: Define the computational graph using tensors x and y\n",
        "  #-----------------------------------------------------------------------------\n",
        "  pass\n",
        "\n",
        "  #-----------------------------------------------------------------------------\n",
        "  # TO-DO: Compute the gradient using tensor dx\n",
        "  #-----------------------------------------------------------------------------\n",
        "  pass\n",
        "  \n",
        "  #-----------------------------------------------------------------------------\n",
        "  # TO-DO: Update x\n",
        "  #-----------------------------------------------------------------------------\n",
        "  pass\n",
        "  \n",
        "  # Print values and append to history:\n",
        "  print(i, x.item(), y.item(), dx.item())\n",
        "  hh.append(x.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x0 = -0.1072\n",
            "0 -0.10719718784093857 0.011491237208247185 -0.21439437568187714\n",
            "1 -0.08575774729251862 0.007354391273111105 -0.17151549458503723\n",
            "2 -0.06860619783401489 0.004706810228526592 -0.13721239566802979\n",
            "3 -0.054884959012269974 0.0030123586766421795 -0.10976991802453995\n",
            "4 -0.04390796646475792 0.0019279095577076077 -0.08781593292951584\n",
            "5 -0.035126373171806335 0.0012338621309027076 -0.07025274634361267\n",
            "6 -0.028101099655032158 0.0007896717870607972 -0.056202199310064316\n",
            "7 -0.022480878978967667 0.0005053899367339909 -0.04496175795793533\n",
            "8 -0.017984703183174133 0.00032344955252483487 -0.03596940636634827\n",
            "9 -0.014387763105332851 0.0002070077316602692 -0.028775526210665703\n",
            "10 -0.011510210111737251 0.00013248494360595942 -0.023020420223474503\n",
            "11 -0.009208167903125286 8.479035750497133e-05 -0.018416335806250572\n",
            "12 -0.007366534322500229 5.426582720247097e-05 -0.014733068645000458\n",
            "13 -0.005893227644264698 3.4730132028926164e-05 -0.011786455288529396\n",
            "14 -0.004714582115411758 2.2227284716791473e-05 -0.009429164230823517\n",
            "15 -0.003771665506064892 1.4225460290617775e-05 -0.007543331012129784\n",
            "16 -0.0030173324048519135 9.104294804274105e-06 -0.006034664809703827\n",
            "17 -0.0024138661101460457 5.826749656989705e-06 -0.004827732220292091\n",
            "18 -0.0019310928182676435 3.7291194985300535e-06 -0.003862185636535287\n",
            "19 -0.0015448742778971791 2.386636424489552e-06 -0.0030897485557943583\n",
            "20 -0.001235899399034679 1.527447352600575e-06 -0.002471798798069358\n",
            "21 -0.0009887195192277431 9.77566287474474e-07 -0.0019774390384554863\n",
            "22 -0.000790975580457598 6.25642371687718e-07 -0.001581951160915196\n",
            "23 -0.0006327804876491427 4.004111531230592e-07 -0.0012655609752982855\n",
            "24 -0.0005062244017608464 2.5626314936744166e-07 -0.0010124488035216928\n",
            "25 -0.0004049795097671449 1.640083979737028e-07 -0.0008099590195342898\n",
            "26 -0.00032398360781371593 1.0496538038751169e-07 -0.0006479672156274319\n",
            "27 -0.0002591868687886745 6.717782952136986e-08 -0.000518373737577349\n",
            "28 -0.00020734951249323785 4.299382183603484e-08 -0.0004146990249864757\n",
            "29 -0.00016587960999459028 2.7516044553976826e-08 -0.00033175921998918056\n",
            "30 -0.00013270368799567223 1.7610268798762263e-08 -0.00026540737599134445\n",
            "31 -0.00010616294457577169 1.1270571143029429e-08 -0.00021232588915154338\n",
            "32 -8.493035420542583e-05 7.2131651762674664e-09 -0.00016986070841085166\n",
            "33 -6.794428918510675e-05 4.616426352299641e-09 -0.0001358885783702135\n",
            "34 -5.4355426982510835e-05 2.954512412500776e-09 -0.00010871085396502167\n",
            "35 -4.348434231360443e-05 1.8908881127543964e-09 -8.696868462720886e-05\n",
            "36 -3.4787473850883543e-05 1.2101682989040796e-09 -6.957494770176709e-05\n",
            "37 -2.7829979444504716e-05 7.745077357235175e-10 -5.565995888900943e-05\n",
            "38 -2.226398282800801e-05 4.956849375403749e-10 -4.452796565601602e-05\n",
            "39 -1.7811185898608528e-05 3.1723834670316364e-10 -3.5622371797217056e-05\n",
            "40 -1.4248949810280465e-05 2.0303257686205e-10 -2.849789962056093e-05\n",
            "41 -1.139915912062861e-05 1.299408219912479e-10 -2.279831824125722e-05\n",
            "42 -9.11932784219971e-06 8.316213689907315e-11 -1.823865568439942e-05\n",
            "43 -7.295462182810297e-06 5.322376914196347e-11 -1.4590924365620594e-05\n",
            "44 -5.836369837197708e-06 3.406321405496904e-11 -1.1672739674395416e-05\n",
            "45 -4.669095687859226e-06 2.1800453803288988e-11 -9.338191375718452e-06\n",
            "46 -3.735276550287381e-06 1.3952290711660709e-11 -7.470553100574762e-06\n",
            "47 -2.9882212402299047e-06 8.929466541185427e-12 -5.976442480459809e-06\n",
            "48 -2.390577037658659e-06 5.7148587077893165e-12 -4.781154075317318e-06\n",
            "49 -1.912461584652192e-06 3.657509191345998e-12 -3.824923169304384e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKRlEVGWWYxl"
      },
      "source": [
        "The following plot shows the convergence of the algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us83p8MUSNUo",
        "outputId": "a8f37797-94fb-4163-ee3d-1f46be999815",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(hh)\n",
        "plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9aQ48l0WioT"
      },
      "source": [
        "### <font color=\"#CA3532\">Exercise 2</font>\n",
        "\n",
        "Re-implement the ``LinearRegressionModel`` class of lab 1 using PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5yU68DhL0cX"
      },
      "source": [
        "class LinearRegressionModel_PyTorch(object):\n",
        "\n",
        "    def __init__(self, d=2):\n",
        "        # Initialize weights and bias:\n",
        "        self.w = torch.tensor(np.random.normal((d, 1)), requires_grad=True) \n",
        "        self.b = torch.tensor(np.random.normal((1, 1)), requires_grad=True) \n",
        "        \n",
        "    def predict(self, x):\n",
        "        #-----------------------------------------------------------------------\n",
        "        # TO-DO block: Compute the model output y\n",
        "        # Note that:\n",
        "        # - x is a Nxd tensor, with N the number of patterns and d the dimension\n",
        "        #   (number of features)\n",
        "        # - y must be a Nx1 tensor\n",
        "        #-----------------------------------------------------------------------\n",
        "        pass\n",
        "        #-----------------------------------------------------------------------\n",
        "        # End of TO-DO block \n",
        "        #-----------------------------------------------------------------------\n",
        "\n",
        "        return y\n",
        "\n",
        "    def compute_gradients(self, x, t):\n",
        "        #-----------------------------------------------------------------------\n",
        "        # TO-DO block: Compute the gradients db and dw of the loss function \n",
        "        # with respect to b and w\n",
        "        # Note that:\n",
        "        # - x is a Nxd tensor, with N the number of patterns and d the dimension\n",
        "        #   (number of features)\n",
        "        # - t is a Nx1 tensor\n",
        "        # - y is a Nx1 tensor\n",
        "        # - The gradient db (eq. dw) must have the same shape as b (eq. w) \n",
        "        #-----------------------------------------------------------------------\n",
        "        pass\n",
        "        #-----------------------------------------------------------------------\n",
        "        # End of TO-DO block \n",
        "        #-----------------------------------------------------------------------\n",
        "        \n",
        "        return db, dw\n",
        "        \n",
        "    def gradient_step(self, x, t, eta):\n",
        "        db, dw = self.compute_gradients(x, t)\n",
        "\n",
        "        #-----------------------------------------------------------------------\n",
        "        # TO-DO block: Update the model parameters b and w\n",
        "        #-----------------------------------------------------------------------\n",
        "        pass\n",
        "        #-----------------------------------------------------------------------\n",
        "        # End of TO-DO block \n",
        "        #-----------------------------------------------------------------------\n",
        "\n",
        "    def fit(self, x, t, eta, num_iters):\n",
        "        loss = np.zeros(num_iters)\n",
        "        for i in range(num_iters):\n",
        "            self.gradient_step(x, t, eta)\n",
        "            loss[i] = self.get_loss(x, t).detach().numpy()\n",
        "        return loss\n",
        "\n",
        "    def get_loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        loss = torch.mean(0.5*(y - torch.tensor(t))*(y - torch.tensor(t)))\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzfaTfEdf7U4"
      },
      "source": [
        "Test the ``predict`` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqUn7HA4L0Oc",
        "outputId": "c963579c-5186-4060-a7fe-9b16449e0341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "dg = dlfbt.DataGeneratorLinear(a=[-5.0, 2.0, -3.0, 2.0])\n",
        "dg.create_dataset(noise=0.0, n=500)\n",
        "\n",
        "linrm = LinearRegressionModel_PyTorch()\n",
        "linrm.w = torch.tensor(dg.a, requires_grad=True)\n",
        "linrm.b = torch.tensor(dg.b, requires_grad=True)\n",
        "\n",
        "y = linrm.predict(dg.x)\n",
        "print(y.shape)                                     # Should be (500, 1)\n",
        "print(np.abs(y.detach().numpy() - dg.t).max())     # Should be 0 or close to 0 \n",
        "print(linrm.get_loss(dg.x, dg.t).detach().numpy()) # Should be 0 or close to 0 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([500, 1])\n",
            "1.4210854715202004e-14\n",
            "5.5226179822259984e-30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG5ACvPWf6tF"
      },
      "source": [
        "Test the ``compute_gradients`` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81n7fG_WdgXS",
        "outputId": "ae863d89-f3be-4a29-b743-769b298a0882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "dg = dlfbt.DataGeneratorLinear(a=[-5.0, 2.0, -3.0, 2.0])\n",
        "dg.create_dataset(n=50000, seed=17)\n",
        "\n",
        "linrm = LinearRegressionModel_PyTorch()\n",
        "linrm.w = torch.tensor(dg.a, requires_grad=True)\n",
        "linrm.b = torch.tensor(dg.b, requires_grad=True)\n",
        "\n",
        "db, dw = linrm.compute_gradients(dg.x, dg.t)\n",
        "print(db.shape) # Should be (1, 1)\n",
        "print(db)       # Should be [[-0.00704326]] approx\n",
        "print(dw.shape) # Should be (4, 1)\n",
        "print(dw)       # Should be [[-0.05353578]\n",
        "                #            [-0.03276935]\n",
        "                #            [-0.00337341]\n",
        "                #            [-0.03293776]] approx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1)\n",
            "[[-0.00704326]]\n",
            "(4, 1)\n",
            "[[-0.05353578]\n",
            " [-0.03276935]\n",
            " [-0.00337341]\n",
            " [-0.03293776]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5y0C9ykgNJh"
      },
      "source": [
        "Test the ``fit`` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzlNX_todf91",
        "outputId": "8e2d5051-8355-43d9-cb4b-b9b142a7b5b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dg = dlfbt.DataGeneratorLinear(a=[2.0, 2.0])\n",
        "dg.create_dataset(n=500, seed=17)\n",
        "\n",
        "linrm = LinearRegressionModel_PyTorch(2)\n",
        "linrm.w = torch.tensor([[-2.0], [-2.0]], dtype=torch.float64, requires_grad=True)\n",
        "linrm.b = torch.tensor([[-3.0]], dtype=torch.float64, requires_grad=True)\n",
        "\n",
        "loss = linrm.fit(dg.x, dg.t, 0.01, 100)\n",
        "print(loss[-1]) # Should be 2.660068435196912 approx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.660068435196912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypJWy5aymaIx"
      },
      "source": [
        "Plot loss vs iteration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJsZhQGmh1Du",
        "outputId": "6528b13f-7e3b-48d2-e7fb-2de36de8db7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(loss)\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beFccWoLQFA-"
      },
      "source": [
        "### <font color=\"#CA3532\">Neural networks in pytorch</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrIWsP8SHcB7"
      },
      "source": [
        "We will build a simple feedforward neural network to classify the digits in the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. The next cell loads the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HhfMBJVQXtC",
        "outputId": "6f438832-20c5-40c0-f3a4-01fc68bd4730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "print(train_images.shape)\n",
        "print(train_labels.shape)\n",
        "print(train_labels)\n",
        "\n",
        "print(test_images.shape)\n",
        "print(test_labels.shape)\n",
        "print(test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "[5 0 4 ... 5 6 8]\n",
            "(10000, 28, 28)\n",
            "(10000,)\n",
            "[7 2 1 ... 4 5 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnuRp4E0Qaoh"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNLT7ehDRV6F"
      },
      "source": [
        "Definition of a class that implements the neural network (only the forward pass):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-l_aNmbQhq4"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x) # Note that we are not explicitly applying a softmax function\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SaWvoqHH0l9"
      },
      "source": [
        "NN instantiation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGO3VYqWHuir",
        "outputId": "295f4858-24f1-45b3-b932-fc5d5b27b565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYDsFV2oIdLl"
      },
      "source": [
        "Network parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p4XzR8WUiI9",
        "outputId": "dd970ac6-2612-4ebc-bcf6-6bb6dbf0fea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "\n",
        "for p in params:\n",
        "  print(p.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "torch.Size([64, 784])\n",
            "torch.Size([64])\n",
            "torch.Size([10, 64])\n",
            "torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vpsL9yRRZE3"
      },
      "source": [
        "Let us apply this network to a sample image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6SaZSOuQob_",
        "outputId": "8e00be2a-2b4d-497e-945a-fb5220f2f607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "print(train_images[1].shape)\n",
        "print(\"target =\", train_labels[1])\n",
        "plt.imshow(train_images[1], cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "# Image to tensor:\n",
        "input = torch.tensor(train_images[1], dtype=torch.float) \n",
        "\n",
        "# Predict:\n",
        "out = net(input)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28)\n",
            "target = 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[-15.8364,  42.3447,  30.1300,  18.1742,   8.2528,  15.0643,  -1.0942,\n",
            "          47.8920, -13.8266,  40.6218]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWgQ-rBbUR7-"
      },
      "source": [
        "Define the optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQrG3GEfRkQf"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPQCb3W0Jmp4"
      },
      "source": [
        "Network training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYSNntVeTHYW",
        "outputId": "f7ee8778-abdc-499b-b4e8-85cb79a09158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "niters = 1000\n",
        "hh = []\n",
        "for it in range(niters):  \n",
        "    # Generate random mini-batch:\n",
        "    ix = np.random.permutation(60000)[:1000]\n",
        "    x = torch.tensor(train_images[ix], dtype=torch.float)\n",
        "    y = torch.tensor(train_labels[ix], dtype=torch.long)\n",
        "\n",
        "    # Reset gradients:\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass:\n",
        "    outputs = net(x)\n",
        "    loss = criterion(outputs, y)\n",
        "\n",
        "    # Backward pass:\n",
        "    loss.backward()\n",
        "\n",
        "    # Update:\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print stats:\n",
        "    print('[%d] loss: %.3f' % (it + 1, loss.item()))\n",
        "    hh.append(loss.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1] loss: 28.105\n",
            "[2] loss: 19.678\n",
            "[3] loss: 17.592\n",
            "[4] loss: 14.034\n",
            "[5] loss: 5.016\n",
            "[6] loss: 2.956\n",
            "[7] loss: 2.094\n",
            "[8] loss: 1.706\n",
            "[9] loss: 1.628\n",
            "[10] loss: 1.580\n",
            "[11] loss: 1.412\n",
            "[12] loss: 1.555\n",
            "[13] loss: 1.579\n",
            "[14] loss: 1.455\n",
            "[15] loss: 1.448\n",
            "[16] loss: 1.485\n",
            "[17] loss: 1.363\n",
            "[18] loss: 1.348\n",
            "[19] loss: 1.245\n",
            "[20] loss: 1.247\n",
            "[21] loss: 1.204\n",
            "[22] loss: 1.167\n",
            "[23] loss: 1.141\n",
            "[24] loss: 1.075\n",
            "[25] loss: 1.111\n",
            "[26] loss: 1.101\n",
            "[27] loss: 1.117\n",
            "[28] loss: 1.043\n",
            "[29] loss: 1.020\n",
            "[30] loss: 1.063\n",
            "[31] loss: 1.037\n",
            "[32] loss: 0.982\n",
            "[33] loss: 0.942\n",
            "[34] loss: 0.933\n",
            "[35] loss: 0.956\n",
            "[36] loss: 0.984\n",
            "[37] loss: 0.941\n",
            "[38] loss: 0.915\n",
            "[39] loss: 0.845\n",
            "[40] loss: 0.901\n",
            "[41] loss: 0.774\n",
            "[42] loss: 0.856\n",
            "[43] loss: 0.866\n",
            "[44] loss: 0.838\n",
            "[45] loss: 0.750\n",
            "[46] loss: 0.787\n",
            "[47] loss: 0.881\n",
            "[48] loss: 0.749\n",
            "[49] loss: 0.781\n",
            "[50] loss: 0.815\n",
            "[51] loss: 0.860\n",
            "[52] loss: 0.727\n",
            "[53] loss: 0.778\n",
            "[54] loss: 0.739\n",
            "[55] loss: 0.773\n",
            "[56] loss: 0.710\n",
            "[57] loss: 0.682\n",
            "[58] loss: 0.662\n",
            "[59] loss: 0.742\n",
            "[60] loss: 0.723\n",
            "[61] loss: 0.706\n",
            "[62] loss: 0.663\n",
            "[63] loss: 0.649\n",
            "[64] loss: 0.709\n",
            "[65] loss: 0.715\n",
            "[66] loss: 0.721\n",
            "[67] loss: 0.632\n",
            "[68] loss: 0.639\n",
            "[69] loss: 0.672\n",
            "[70] loss: 0.645\n",
            "[71] loss: 0.684\n",
            "[72] loss: 0.611\n",
            "[73] loss: 0.668\n",
            "[74] loss: 0.679\n",
            "[75] loss: 0.653\n",
            "[76] loss: 0.680\n",
            "[77] loss: 0.619\n",
            "[78] loss: 0.605\n",
            "[79] loss: 0.541\n",
            "[80] loss: 0.637\n",
            "[81] loss: 0.614\n",
            "[82] loss: 0.681\n",
            "[83] loss: 0.515\n",
            "[84] loss: 0.585\n",
            "[85] loss: 0.528\n",
            "[86] loss: 0.582\n",
            "[87] loss: 0.585\n",
            "[88] loss: 0.517\n",
            "[89] loss: 0.577\n",
            "[90] loss: 0.511\n",
            "[91] loss: 0.545\n",
            "[92] loss: 0.562\n",
            "[93] loss: 0.602\n",
            "[94] loss: 0.469\n",
            "[95] loss: 0.613\n",
            "[96] loss: 0.553\n",
            "[97] loss: 0.475\n",
            "[98] loss: 0.570\n",
            "[99] loss: 0.436\n",
            "[100] loss: 0.517\n",
            "[101] loss: 0.506\n",
            "[102] loss: 0.497\n",
            "[103] loss: 0.474\n",
            "[104] loss: 0.504\n",
            "[105] loss: 0.474\n",
            "[106] loss: 0.519\n",
            "[107] loss: 0.499\n",
            "[108] loss: 0.511\n",
            "[109] loss: 0.440\n",
            "[110] loss: 0.521\n",
            "[111] loss: 0.488\n",
            "[112] loss: 0.425\n",
            "[113] loss: 0.500\n",
            "[114] loss: 0.443\n",
            "[115] loss: 0.368\n",
            "[116] loss: 0.540\n",
            "[117] loss: 0.488\n",
            "[118] loss: 0.496\n",
            "[119] loss: 0.560\n",
            "[120] loss: 0.497\n",
            "[121] loss: 0.387\n",
            "[122] loss: 0.502\n",
            "[123] loss: 0.530\n",
            "[124] loss: 0.522\n",
            "[125] loss: 0.500\n",
            "[126] loss: 0.488\n",
            "[127] loss: 0.490\n",
            "[128] loss: 0.499\n",
            "[129] loss: 0.412\n",
            "[130] loss: 0.450\n",
            "[131] loss: 0.414\n",
            "[132] loss: 0.368\n",
            "[133] loss: 0.461\n",
            "[134] loss: 0.472\n",
            "[135] loss: 0.443\n",
            "[136] loss: 0.422\n",
            "[137] loss: 0.378\n",
            "[138] loss: 0.447\n",
            "[139] loss: 0.412\n",
            "[140] loss: 0.471\n",
            "[141] loss: 0.343\n",
            "[142] loss: 0.414\n",
            "[143] loss: 0.328\n",
            "[144] loss: 0.450\n",
            "[145] loss: 0.450\n",
            "[146] loss: 0.442\n",
            "[147] loss: 0.447\n",
            "[148] loss: 0.439\n",
            "[149] loss: 0.398\n",
            "[150] loss: 0.500\n",
            "[151] loss: 0.395\n",
            "[152] loss: 0.399\n",
            "[153] loss: 0.428\n",
            "[154] loss: 0.394\n",
            "[155] loss: 0.467\n",
            "[156] loss: 0.395\n",
            "[157] loss: 0.330\n",
            "[158] loss: 0.394\n",
            "[159] loss: 0.416\n",
            "[160] loss: 0.380\n",
            "[161] loss: 0.472\n",
            "[162] loss: 0.360\n",
            "[163] loss: 0.474\n",
            "[164] loss: 0.425\n",
            "[165] loss: 0.317\n",
            "[166] loss: 0.422\n",
            "[167] loss: 0.396\n",
            "[168] loss: 0.391\n",
            "[169] loss: 0.434\n",
            "[170] loss: 0.394\n",
            "[171] loss: 0.391\n",
            "[172] loss: 0.373\n",
            "[173] loss: 0.347\n",
            "[174] loss: 0.358\n",
            "[175] loss: 0.418\n",
            "[176] loss: 0.372\n",
            "[177] loss: 0.349\n",
            "[178] loss: 0.464\n",
            "[179] loss: 0.389\n",
            "[180] loss: 0.344\n",
            "[181] loss: 0.413\n",
            "[182] loss: 0.389\n",
            "[183] loss: 0.378\n",
            "[184] loss: 0.349\n",
            "[185] loss: 0.396\n",
            "[186] loss: 0.340\n",
            "[187] loss: 0.360\n",
            "[188] loss: 0.333\n",
            "[189] loss: 0.393\n",
            "[190] loss: 0.350\n",
            "[191] loss: 0.412\n",
            "[192] loss: 0.311\n",
            "[193] loss: 0.372\n",
            "[194] loss: 0.361\n",
            "[195] loss: 0.343\n",
            "[196] loss: 0.335\n",
            "[197] loss: 0.358\n",
            "[198] loss: 0.336\n",
            "[199] loss: 0.285\n",
            "[200] loss: 0.384\n",
            "[201] loss: 0.394\n",
            "[202] loss: 0.374\n",
            "[203] loss: 0.359\n",
            "[204] loss: 0.408\n",
            "[205] loss: 0.318\n",
            "[206] loss: 0.295\n",
            "[207] loss: 0.279\n",
            "[208] loss: 0.344\n",
            "[209] loss: 0.291\n",
            "[210] loss: 0.351\n",
            "[211] loss: 0.357\n",
            "[212] loss: 0.332\n",
            "[213] loss: 0.295\n",
            "[214] loss: 0.332\n",
            "[215] loss: 0.313\n",
            "[216] loss: 0.338\n",
            "[217] loss: 0.368\n",
            "[218] loss: 0.275\n",
            "[219] loss: 0.313\n",
            "[220] loss: 0.362\n",
            "[221] loss: 0.325\n",
            "[222] loss: 0.333\n",
            "[223] loss: 0.334\n",
            "[224] loss: 0.346\n",
            "[225] loss: 0.318\n",
            "[226] loss: 0.321\n",
            "[227] loss: 0.313\n",
            "[228] loss: 0.305\n",
            "[229] loss: 0.332\n",
            "[230] loss: 0.358\n",
            "[231] loss: 0.336\n",
            "[232] loss: 0.345\n",
            "[233] loss: 0.333\n",
            "[234] loss: 0.324\n",
            "[235] loss: 0.289\n",
            "[236] loss: 0.285\n",
            "[237] loss: 0.323\n",
            "[238] loss: 0.302\n",
            "[239] loss: 0.363\n",
            "[240] loss: 0.291\n",
            "[241] loss: 0.296\n",
            "[242] loss: 0.328\n",
            "[243] loss: 0.355\n",
            "[244] loss: 0.286\n",
            "[245] loss: 0.316\n",
            "[246] loss: 0.360\n",
            "[247] loss: 0.350\n",
            "[248] loss: 0.326\n",
            "[249] loss: 0.416\n",
            "[250] loss: 0.260\n",
            "[251] loss: 0.319\n",
            "[252] loss: 0.350\n",
            "[253] loss: 0.353\n",
            "[254] loss: 0.266\n",
            "[255] loss: 0.328\n",
            "[256] loss: 0.300\n",
            "[257] loss: 0.357\n",
            "[258] loss: 0.290\n",
            "[259] loss: 0.334\n",
            "[260] loss: 0.308\n",
            "[261] loss: 0.304\n",
            "[262] loss: 0.270\n",
            "[263] loss: 0.318\n",
            "[264] loss: 0.288\n",
            "[265] loss: 0.320\n",
            "[266] loss: 0.289\n",
            "[267] loss: 0.293\n",
            "[268] loss: 0.311\n",
            "[269] loss: 0.305\n",
            "[270] loss: 0.291\n",
            "[271] loss: 0.327\n",
            "[272] loss: 0.289\n",
            "[273] loss: 0.303\n",
            "[274] loss: 0.319\n",
            "[275] loss: 0.347\n",
            "[276] loss: 0.330\n",
            "[277] loss: 0.283\n",
            "[278] loss: 0.262\n",
            "[279] loss: 0.294\n",
            "[280] loss: 0.302\n",
            "[281] loss: 0.320\n",
            "[282] loss: 0.323\n",
            "[283] loss: 0.321\n",
            "[284] loss: 0.327\n",
            "[285] loss: 0.311\n",
            "[286] loss: 0.319\n",
            "[287] loss: 0.320\n",
            "[288] loss: 0.330\n",
            "[289] loss: 0.317\n",
            "[290] loss: 0.295\n",
            "[291] loss: 0.323\n",
            "[292] loss: 0.336\n",
            "[293] loss: 0.316\n",
            "[294] loss: 0.279\n",
            "[295] loss: 0.295\n",
            "[296] loss: 0.301\n",
            "[297] loss: 0.308\n",
            "[298] loss: 0.320\n",
            "[299] loss: 0.374\n",
            "[300] loss: 0.243\n",
            "[301] loss: 0.308\n",
            "[302] loss: 0.296\n",
            "[303] loss: 0.300\n",
            "[304] loss: 0.306\n",
            "[305] loss: 0.285\n",
            "[306] loss: 0.394\n",
            "[307] loss: 0.265\n",
            "[308] loss: 0.297\n",
            "[309] loss: 0.281\n",
            "[310] loss: 0.349\n",
            "[311] loss: 0.331\n",
            "[312] loss: 0.334\n",
            "[313] loss: 0.237\n",
            "[314] loss: 0.272\n",
            "[315] loss: 0.346\n",
            "[316] loss: 0.267\n",
            "[317] loss: 0.329\n",
            "[318] loss: 0.310\n",
            "[319] loss: 0.243\n",
            "[320] loss: 0.284\n",
            "[321] loss: 0.290\n",
            "[322] loss: 0.275\n",
            "[323] loss: 0.291\n",
            "[324] loss: 0.277\n",
            "[325] loss: 0.319\n",
            "[326] loss: 0.255\n",
            "[327] loss: 0.328\n",
            "[328] loss: 0.309\n",
            "[329] loss: 0.298\n",
            "[330] loss: 0.280\n",
            "[331] loss: 0.298\n",
            "[332] loss: 0.310\n",
            "[333] loss: 0.310\n",
            "[334] loss: 0.270\n",
            "[335] loss: 0.259\n",
            "[336] loss: 0.301\n",
            "[337] loss: 0.304\n",
            "[338] loss: 0.330\n",
            "[339] loss: 0.320\n",
            "[340] loss: 0.289\n",
            "[341] loss: 0.328\n",
            "[342] loss: 0.258\n",
            "[343] loss: 0.302\n",
            "[344] loss: 0.294\n",
            "[345] loss: 0.227\n",
            "[346] loss: 0.232\n",
            "[347] loss: 0.312\n",
            "[348] loss: 0.246\n",
            "[349] loss: 0.266\n",
            "[350] loss: 0.298\n",
            "[351] loss: 0.272\n",
            "[352] loss: 0.250\n",
            "[353] loss: 0.223\n",
            "[354] loss: 0.267\n",
            "[355] loss: 0.312\n",
            "[356] loss: 0.253\n",
            "[357] loss: 0.282\n",
            "[358] loss: 0.222\n",
            "[359] loss: 0.290\n",
            "[360] loss: 0.303\n",
            "[361] loss: 0.263\n",
            "[362] loss: 0.289\n",
            "[363] loss: 0.296\n",
            "[364] loss: 0.290\n",
            "[365] loss: 0.300\n",
            "[366] loss: 0.233\n",
            "[367] loss: 0.258\n",
            "[368] loss: 0.235\n",
            "[369] loss: 0.363\n",
            "[370] loss: 0.279\n",
            "[371] loss: 0.237\n",
            "[372] loss: 0.295\n",
            "[373] loss: 0.209\n",
            "[374] loss: 0.291\n",
            "[375] loss: 0.295\n",
            "[376] loss: 0.262\n",
            "[377] loss: 0.301\n",
            "[378] loss: 0.294\n",
            "[379] loss: 0.276\n",
            "[380] loss: 0.284\n",
            "[381] loss: 0.259\n",
            "[382] loss: 0.264\n",
            "[383] loss: 0.223\n",
            "[384] loss: 0.271\n",
            "[385] loss: 0.288\n",
            "[386] loss: 0.239\n",
            "[387] loss: 0.257\n",
            "[388] loss: 0.235\n",
            "[389] loss: 0.241\n",
            "[390] loss: 0.275\n",
            "[391] loss: 0.269\n",
            "[392] loss: 0.312\n",
            "[393] loss: 0.298\n",
            "[394] loss: 0.254\n",
            "[395] loss: 0.259\n",
            "[396] loss: 0.307\n",
            "[397] loss: 0.214\n",
            "[398] loss: 0.268\n",
            "[399] loss: 0.248\n",
            "[400] loss: 0.262\n",
            "[401] loss: 0.295\n",
            "[402] loss: 0.289\n",
            "[403] loss: 0.285\n",
            "[404] loss: 0.245\n",
            "[405] loss: 0.271\n",
            "[406] loss: 0.287\n",
            "[407] loss: 0.301\n",
            "[408] loss: 0.269\n",
            "[409] loss: 0.311\n",
            "[410] loss: 0.282\n",
            "[411] loss: 0.251\n",
            "[412] loss: 0.267\n",
            "[413] loss: 0.331\n",
            "[414] loss: 0.273\n",
            "[415] loss: 0.262\n",
            "[416] loss: 0.262\n",
            "[417] loss: 0.300\n",
            "[418] loss: 0.308\n",
            "[419] loss: 0.253\n",
            "[420] loss: 0.196\n",
            "[421] loss: 0.243\n",
            "[422] loss: 0.232\n",
            "[423] loss: 0.328\n",
            "[424] loss: 0.245\n",
            "[425] loss: 0.297\n",
            "[426] loss: 0.299\n",
            "[427] loss: 0.280\n",
            "[428] loss: 0.309\n",
            "[429] loss: 0.246\n",
            "[430] loss: 0.268\n",
            "[431] loss: 0.268\n",
            "[432] loss: 0.218\n",
            "[433] loss: 0.269\n",
            "[434] loss: 0.243\n",
            "[435] loss: 0.244\n",
            "[436] loss: 0.246\n",
            "[437] loss: 0.251\n",
            "[438] loss: 0.204\n",
            "[439] loss: 0.271\n",
            "[440] loss: 0.284\n",
            "[441] loss: 0.291\n",
            "[442] loss: 0.259\n",
            "[443] loss: 0.278\n",
            "[444] loss: 0.262\n",
            "[445] loss: 0.247\n",
            "[446] loss: 0.280\n",
            "[447] loss: 0.251\n",
            "[448] loss: 0.306\n",
            "[449] loss: 0.229\n",
            "[450] loss: 0.240\n",
            "[451] loss: 0.296\n",
            "[452] loss: 0.315\n",
            "[453] loss: 0.214\n",
            "[454] loss: 0.282\n",
            "[455] loss: 0.260\n",
            "[456] loss: 0.271\n",
            "[457] loss: 0.303\n",
            "[458] loss: 0.317\n",
            "[459] loss: 0.263\n",
            "[460] loss: 0.270\n",
            "[461] loss: 0.257\n",
            "[462] loss: 0.235\n",
            "[463] loss: 0.269\n",
            "[464] loss: 0.193\n",
            "[465] loss: 0.294\n",
            "[466] loss: 0.253\n",
            "[467] loss: 0.282\n",
            "[468] loss: 0.237\n",
            "[469] loss: 0.246\n",
            "[470] loss: 0.269\n",
            "[471] loss: 0.235\n",
            "[472] loss: 0.231\n",
            "[473] loss: 0.273\n",
            "[474] loss: 0.308\n",
            "[475] loss: 0.247\n",
            "[476] loss: 0.286\n",
            "[477] loss: 0.277\n",
            "[478] loss: 0.238\n",
            "[479] loss: 0.285\n",
            "[480] loss: 0.231\n",
            "[481] loss: 0.266\n",
            "[482] loss: 0.279\n",
            "[483] loss: 0.229\n",
            "[484] loss: 0.265\n",
            "[485] loss: 0.222\n",
            "[486] loss: 0.236\n",
            "[487] loss: 0.296\n",
            "[488] loss: 0.206\n",
            "[489] loss: 0.237\n",
            "[490] loss: 0.235\n",
            "[491] loss: 0.237\n",
            "[492] loss: 0.272\n",
            "[493] loss: 0.265\n",
            "[494] loss: 0.261\n",
            "[495] loss: 0.220\n",
            "[496] loss: 0.244\n",
            "[497] loss: 0.232\n",
            "[498] loss: 0.281\n",
            "[499] loss: 0.256\n",
            "[500] loss: 0.258\n",
            "[501] loss: 0.278\n",
            "[502] loss: 0.262\n",
            "[503] loss: 0.222\n",
            "[504] loss: 0.266\n",
            "[505] loss: 0.260\n",
            "[506] loss: 0.231\n",
            "[507] loss: 0.219\n",
            "[508] loss: 0.249\n",
            "[509] loss: 0.274\n",
            "[510] loss: 0.231\n",
            "[511] loss: 0.277\n",
            "[512] loss: 0.217\n",
            "[513] loss: 0.205\n",
            "[514] loss: 0.247\n",
            "[515] loss: 0.242\n",
            "[516] loss: 0.276\n",
            "[517] loss: 0.231\n",
            "[518] loss: 0.216\n",
            "[519] loss: 0.196\n",
            "[520] loss: 0.217\n",
            "[521] loss: 0.215\n",
            "[522] loss: 0.293\n",
            "[523] loss: 0.274\n",
            "[524] loss: 0.267\n",
            "[525] loss: 0.203\n",
            "[526] loss: 0.245\n",
            "[527] loss: 0.250\n",
            "[528] loss: 0.250\n",
            "[529] loss: 0.256\n",
            "[530] loss: 0.222\n",
            "[531] loss: 0.241\n",
            "[532] loss: 0.237\n",
            "[533] loss: 0.274\n",
            "[534] loss: 0.227\n",
            "[535] loss: 0.258\n",
            "[536] loss: 0.196\n",
            "[537] loss: 0.235\n",
            "[538] loss: 0.241\n",
            "[539] loss: 0.250\n",
            "[540] loss: 0.241\n",
            "[541] loss: 0.219\n",
            "[542] loss: 0.214\n",
            "[543] loss: 0.214\n",
            "[544] loss: 0.224\n",
            "[545] loss: 0.233\n",
            "[546] loss: 0.240\n",
            "[547] loss: 0.208\n",
            "[548] loss: 0.246\n",
            "[549] loss: 0.249\n",
            "[550] loss: 0.250\n",
            "[551] loss: 0.254\n",
            "[552] loss: 0.229\n",
            "[553] loss: 0.283\n",
            "[554] loss: 0.229\n",
            "[555] loss: 0.261\n",
            "[556] loss: 0.263\n",
            "[557] loss: 0.283\n",
            "[558] loss: 0.204\n",
            "[559] loss: 0.296\n",
            "[560] loss: 0.237\n",
            "[561] loss: 0.287\n",
            "[562] loss: 0.259\n",
            "[563] loss: 0.236\n",
            "[564] loss: 0.276\n",
            "[565] loss: 0.286\n",
            "[566] loss: 0.290\n",
            "[567] loss: 0.262\n",
            "[568] loss: 0.245\n",
            "[569] loss: 0.250\n",
            "[570] loss: 0.320\n",
            "[571] loss: 0.220\n",
            "[572] loss: 0.253\n",
            "[573] loss: 0.226\n",
            "[574] loss: 0.266\n",
            "[575] loss: 0.282\n",
            "[576] loss: 0.276\n",
            "[577] loss: 0.246\n",
            "[578] loss: 0.312\n",
            "[579] loss: 0.234\n",
            "[580] loss: 0.276\n",
            "[581] loss: 0.211\n",
            "[582] loss: 0.265\n",
            "[583] loss: 0.263\n",
            "[584] loss: 0.241\n",
            "[585] loss: 0.263\n",
            "[586] loss: 0.231\n",
            "[587] loss: 0.231\n",
            "[588] loss: 0.240\n",
            "[589] loss: 0.198\n",
            "[590] loss: 0.240\n",
            "[591] loss: 0.260\n",
            "[592] loss: 0.213\n",
            "[593] loss: 0.196\n",
            "[594] loss: 0.244\n",
            "[595] loss: 0.291\n",
            "[596] loss: 0.287\n",
            "[597] loss: 0.200\n",
            "[598] loss: 0.220\n",
            "[599] loss: 0.216\n",
            "[600] loss: 0.275\n",
            "[601] loss: 0.260\n",
            "[602] loss: 0.251\n",
            "[603] loss: 0.278\n",
            "[604] loss: 0.247\n",
            "[605] loss: 0.220\n",
            "[606] loss: 0.252\n",
            "[607] loss: 0.193\n",
            "[608] loss: 0.286\n",
            "[609] loss: 0.308\n",
            "[610] loss: 0.246\n",
            "[611] loss: 0.195\n",
            "[612] loss: 0.234\n",
            "[613] loss: 0.238\n",
            "[614] loss: 0.234\n",
            "[615] loss: 0.225\n",
            "[616] loss: 0.255\n",
            "[617] loss: 0.223\n",
            "[618] loss: 0.273\n",
            "[619] loss: 0.252\n",
            "[620] loss: 0.209\n",
            "[621] loss: 0.259\n",
            "[622] loss: 0.245\n",
            "[623] loss: 0.250\n",
            "[624] loss: 0.247\n",
            "[625] loss: 0.222\n",
            "[626] loss: 0.225\n",
            "[627] loss: 0.265\n",
            "[628] loss: 0.231\n",
            "[629] loss: 0.260\n",
            "[630] loss: 0.230\n",
            "[631] loss: 0.204\n",
            "[632] loss: 0.290\n",
            "[633] loss: 0.199\n",
            "[634] loss: 0.216\n",
            "[635] loss: 0.235\n",
            "[636] loss: 0.265\n",
            "[637] loss: 0.229\n",
            "[638] loss: 0.242\n",
            "[639] loss: 0.247\n",
            "[640] loss: 0.219\n",
            "[641] loss: 0.269\n",
            "[642] loss: 0.185\n",
            "[643] loss: 0.236\n",
            "[644] loss: 0.268\n",
            "[645] loss: 0.257\n",
            "[646] loss: 0.208\n",
            "[647] loss: 0.238\n",
            "[648] loss: 0.207\n",
            "[649] loss: 0.205\n",
            "[650] loss: 0.249\n",
            "[651] loss: 0.220\n",
            "[652] loss: 0.193\n",
            "[653] loss: 0.219\n",
            "[654] loss: 0.207\n",
            "[655] loss: 0.229\n",
            "[656] loss: 0.194\n",
            "[657] loss: 0.278\n",
            "[658] loss: 0.240\n",
            "[659] loss: 0.234\n",
            "[660] loss: 0.254\n",
            "[661] loss: 0.219\n",
            "[662] loss: 0.214\n",
            "[663] loss: 0.196\n",
            "[664] loss: 0.236\n",
            "[665] loss: 0.206\n",
            "[666] loss: 0.226\n",
            "[667] loss: 0.186\n",
            "[668] loss: 0.207\n",
            "[669] loss: 0.311\n",
            "[670] loss: 0.218\n",
            "[671] loss: 0.202\n",
            "[672] loss: 0.242\n",
            "[673] loss: 0.231\n",
            "[674] loss: 0.220\n",
            "[675] loss: 0.209\n",
            "[676] loss: 0.175\n",
            "[677] loss: 0.290\n",
            "[678] loss: 0.194\n",
            "[679] loss: 0.226\n",
            "[680] loss: 0.219\n",
            "[681] loss: 0.273\n",
            "[682] loss: 0.240\n",
            "[683] loss: 0.186\n",
            "[684] loss: 0.187\n",
            "[685] loss: 0.233\n",
            "[686] loss: 0.260\n",
            "[687] loss: 0.284\n",
            "[688] loss: 0.252\n",
            "[689] loss: 0.190\n",
            "[690] loss: 0.207\n",
            "[691] loss: 0.238\n",
            "[692] loss: 0.247\n",
            "[693] loss: 0.208\n",
            "[694] loss: 0.205\n",
            "[695] loss: 0.229\n",
            "[696] loss: 0.225\n",
            "[697] loss: 0.238\n",
            "[698] loss: 0.208\n",
            "[699] loss: 0.238\n",
            "[700] loss: 0.201\n",
            "[701] loss: 0.235\n",
            "[702] loss: 0.235\n",
            "[703] loss: 0.194\n",
            "[704] loss: 0.267\n",
            "[705] loss: 0.209\n",
            "[706] loss: 0.201\n",
            "[707] loss: 0.295\n",
            "[708] loss: 0.229\n",
            "[709] loss: 0.200\n",
            "[710] loss: 0.226\n",
            "[711] loss: 0.218\n",
            "[712] loss: 0.189\n",
            "[713] loss: 0.217\n",
            "[714] loss: 0.225\n",
            "[715] loss: 0.212\n",
            "[716] loss: 0.262\n",
            "[717] loss: 0.200\n",
            "[718] loss: 0.253\n",
            "[719] loss: 0.251\n",
            "[720] loss: 0.237\n",
            "[721] loss: 0.275\n",
            "[722] loss: 0.240\n",
            "[723] loss: 0.242\n",
            "[724] loss: 0.211\n",
            "[725] loss: 0.271\n",
            "[726] loss: 0.228\n",
            "[727] loss: 0.230\n",
            "[728] loss: 0.228\n",
            "[729] loss: 0.235\n",
            "[730] loss: 0.235\n",
            "[731] loss: 0.254\n",
            "[732] loss: 0.255\n",
            "[733] loss: 0.275\n",
            "[734] loss: 0.209\n",
            "[735] loss: 0.205\n",
            "[736] loss: 0.206\n",
            "[737] loss: 0.219\n",
            "[738] loss: 0.228\n",
            "[739] loss: 0.200\n",
            "[740] loss: 0.197\n",
            "[741] loss: 0.232\n",
            "[742] loss: 0.215\n",
            "[743] loss: 0.226\n",
            "[744] loss: 0.190\n",
            "[745] loss: 0.215\n",
            "[746] loss: 0.190\n",
            "[747] loss: 0.198\n",
            "[748] loss: 0.206\n",
            "[749] loss: 0.235\n",
            "[750] loss: 0.250\n",
            "[751] loss: 0.225\n",
            "[752] loss: 0.237\n",
            "[753] loss: 0.225\n",
            "[754] loss: 0.210\n",
            "[755] loss: 0.204\n",
            "[756] loss: 0.202\n",
            "[757] loss: 0.190\n",
            "[758] loss: 0.218\n",
            "[759] loss: 0.209\n",
            "[760] loss: 0.255\n",
            "[761] loss: 0.218\n",
            "[762] loss: 0.247\n",
            "[763] loss: 0.172\n",
            "[764] loss: 0.189\n",
            "[765] loss: 0.254\n",
            "[766] loss: 0.286\n",
            "[767] loss: 0.213\n",
            "[768] loss: 0.225\n",
            "[769] loss: 0.245\n",
            "[770] loss: 0.202\n",
            "[771] loss: 0.207\n",
            "[772] loss: 0.228\n",
            "[773] loss: 0.202\n",
            "[774] loss: 0.247\n",
            "[775] loss: 0.208\n",
            "[776] loss: 0.196\n",
            "[777] loss: 0.233\n",
            "[778] loss: 0.235\n",
            "[779] loss: 0.242\n",
            "[780] loss: 0.211\n",
            "[781] loss: 0.276\n",
            "[782] loss: 0.247\n",
            "[783] loss: 0.216\n",
            "[784] loss: 0.229\n",
            "[785] loss: 0.232\n",
            "[786] loss: 0.277\n",
            "[787] loss: 0.227\n",
            "[788] loss: 0.214\n",
            "[789] loss: 0.158\n",
            "[790] loss: 0.215\n",
            "[791] loss: 0.209\n",
            "[792] loss: 0.240\n",
            "[793] loss: 0.181\n",
            "[794] loss: 0.223\n",
            "[795] loss: 0.185\n",
            "[796] loss: 0.203\n",
            "[797] loss: 0.208\n",
            "[798] loss: 0.184\n",
            "[799] loss: 0.234\n",
            "[800] loss: 0.216\n",
            "[801] loss: 0.243\n",
            "[802] loss: 0.237\n",
            "[803] loss: 0.207\n",
            "[804] loss: 0.263\n",
            "[805] loss: 0.249\n",
            "[806] loss: 0.213\n",
            "[807] loss: 0.213\n",
            "[808] loss: 0.190\n",
            "[809] loss: 0.189\n",
            "[810] loss: 0.178\n",
            "[811] loss: 0.288\n",
            "[812] loss: 0.227\n",
            "[813] loss: 0.171\n",
            "[814] loss: 0.200\n",
            "[815] loss: 0.203\n",
            "[816] loss: 0.201\n",
            "[817] loss: 0.220\n",
            "[818] loss: 0.257\n",
            "[819] loss: 0.215\n",
            "[820] loss: 0.175\n",
            "[821] loss: 0.234\n",
            "[822] loss: 0.238\n",
            "[823] loss: 0.231\n",
            "[824] loss: 0.201\n",
            "[825] loss: 0.237\n",
            "[826] loss: 0.248\n",
            "[827] loss: 0.201\n",
            "[828] loss: 0.169\n",
            "[829] loss: 0.205\n",
            "[830] loss: 0.213\n",
            "[831] loss: 0.197\n",
            "[832] loss: 0.230\n",
            "[833] loss: 0.238\n",
            "[834] loss: 0.218\n",
            "[835] loss: 0.180\n",
            "[836] loss: 0.211\n",
            "[837] loss: 0.203\n",
            "[838] loss: 0.197\n",
            "[839] loss: 0.193\n",
            "[840] loss: 0.237\n",
            "[841] loss: 0.240\n",
            "[842] loss: 0.244\n",
            "[843] loss: 0.236\n",
            "[844] loss: 0.198\n",
            "[845] loss: 0.222\n",
            "[846] loss: 0.198\n",
            "[847] loss: 0.210\n",
            "[848] loss: 0.220\n",
            "[849] loss: 0.277\n",
            "[850] loss: 0.212\n",
            "[851] loss: 0.206\n",
            "[852] loss: 0.214\n",
            "[853] loss: 0.225\n",
            "[854] loss: 0.191\n",
            "[855] loss: 0.202\n",
            "[856] loss: 0.212\n",
            "[857] loss: 0.184\n",
            "[858] loss: 0.208\n",
            "[859] loss: 0.269\n",
            "[860] loss: 0.236\n",
            "[861] loss: 0.204\n",
            "[862] loss: 0.235\n",
            "[863] loss: 0.174\n",
            "[864] loss: 0.212\n",
            "[865] loss: 0.225\n",
            "[866] loss: 0.222\n",
            "[867] loss: 0.137\n",
            "[868] loss: 0.179\n",
            "[869] loss: 0.237\n",
            "[870] loss: 0.281\n",
            "[871] loss: 0.185\n",
            "[872] loss: 0.200\n",
            "[873] loss: 0.249\n",
            "[874] loss: 0.179\n",
            "[875] loss: 0.226\n",
            "[876] loss: 0.210\n",
            "[877] loss: 0.189\n",
            "[878] loss: 0.220\n",
            "[879] loss: 0.239\n",
            "[880] loss: 0.272\n",
            "[881] loss: 0.235\n",
            "[882] loss: 0.236\n",
            "[883] loss: 0.217\n",
            "[884] loss: 0.199\n",
            "[885] loss: 0.186\n",
            "[886] loss: 0.236\n",
            "[887] loss: 0.176\n",
            "[888] loss: 0.193\n",
            "[889] loss: 0.242\n",
            "[890] loss: 0.200\n",
            "[891] loss: 0.222\n",
            "[892] loss: 0.226\n",
            "[893] loss: 0.230\n",
            "[894] loss: 0.222\n",
            "[895] loss: 0.235\n",
            "[896] loss: 0.183\n",
            "[897] loss: 0.203\n",
            "[898] loss: 0.192\n",
            "[899] loss: 0.219\n",
            "[900] loss: 0.223\n",
            "[901] loss: 0.169\n",
            "[902] loss: 0.188\n",
            "[903] loss: 0.200\n",
            "[904] loss: 0.197\n",
            "[905] loss: 0.204\n",
            "[906] loss: 0.213\n",
            "[907] loss: 0.248\n",
            "[908] loss: 0.245\n",
            "[909] loss: 0.236\n",
            "[910] loss: 0.243\n",
            "[911] loss: 0.207\n",
            "[912] loss: 0.172\n",
            "[913] loss: 0.205\n",
            "[914] loss: 0.224\n",
            "[915] loss: 0.201\n",
            "[916] loss: 0.192\n",
            "[917] loss: 0.194\n",
            "[918] loss: 0.202\n",
            "[919] loss: 0.243\n",
            "[920] loss: 0.221\n",
            "[921] loss: 0.272\n",
            "[922] loss: 0.228\n",
            "[923] loss: 0.194\n",
            "[924] loss: 0.188\n",
            "[925] loss: 0.194\n",
            "[926] loss: 0.195\n",
            "[927] loss: 0.166\n",
            "[928] loss: 0.215\n",
            "[929] loss: 0.224\n",
            "[930] loss: 0.243\n",
            "[931] loss: 0.250\n",
            "[932] loss: 0.257\n",
            "[933] loss: 0.201\n",
            "[934] loss: 0.158\n",
            "[935] loss: 0.219\n",
            "[936] loss: 0.206\n",
            "[937] loss: 0.175\n",
            "[938] loss: 0.174\n",
            "[939] loss: 0.149\n",
            "[940] loss: 0.194\n",
            "[941] loss: 0.194\n",
            "[942] loss: 0.213\n",
            "[943] loss: 0.240\n",
            "[944] loss: 0.210\n",
            "[945] loss: 0.224\n",
            "[946] loss: 0.237\n",
            "[947] loss: 0.211\n",
            "[948] loss: 0.165\n",
            "[949] loss: 0.206\n",
            "[950] loss: 0.214\n",
            "[951] loss: 0.196\n",
            "[952] loss: 0.212\n",
            "[953] loss: 0.266\n",
            "[954] loss: 0.165\n",
            "[955] loss: 0.205\n",
            "[956] loss: 0.223\n",
            "[957] loss: 0.196\n",
            "[958] loss: 0.215\n",
            "[959] loss: 0.150\n",
            "[960] loss: 0.219\n",
            "[961] loss: 0.231\n",
            "[962] loss: 0.233\n",
            "[963] loss: 0.210\n",
            "[964] loss: 0.216\n",
            "[965] loss: 0.180\n",
            "[966] loss: 0.256\n",
            "[967] loss: 0.202\n",
            "[968] loss: 0.204\n",
            "[969] loss: 0.228\n",
            "[970] loss: 0.215\n",
            "[971] loss: 0.249\n",
            "[972] loss: 0.199\n",
            "[973] loss: 0.193\n",
            "[974] loss: 0.168\n",
            "[975] loss: 0.183\n",
            "[976] loss: 0.186\n",
            "[977] loss: 0.185\n",
            "[978] loss: 0.205\n",
            "[979] loss: 0.172\n",
            "[980] loss: 0.197\n",
            "[981] loss: 0.207\n",
            "[982] loss: 0.207\n",
            "[983] loss: 0.267\n",
            "[984] loss: 0.185\n",
            "[985] loss: 0.186\n",
            "[986] loss: 0.205\n",
            "[987] loss: 0.206\n",
            "[988] loss: 0.218\n",
            "[989] loss: 0.175\n",
            "[990] loss: 0.256\n",
            "[991] loss: 0.216\n",
            "[992] loss: 0.203\n",
            "[993] loss: 0.149\n",
            "[994] loss: 0.225\n",
            "[995] loss: 0.189\n",
            "[996] loss: 0.224\n",
            "[997] loss: 0.224\n",
            "[998] loss: 0.189\n",
            "[999] loss: 0.187\n",
            "[1000] loss: 0.166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xs1ZhWcoyaz"
      },
      "source": [
        "Plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5sVcA63V3gI",
        "outputId": "84c7527f-813f-4899-bd1d-5dac25789536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(hh)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRA0N2J9p8jN"
      },
      "source": [
        "Let us classify the same image after training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ps2jawvLWPP",
        "outputId": "8d4accc6-5bad-4779-b1ec-d6cccd91366c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "print(train_images[1].shape)\n",
        "print(\"target =\", train_labels[1])\n",
        "plt.imshow(train_images[1], cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "# Image to tensor:\n",
        "input = torch.tensor(train_images[1], dtype=torch.float) \n",
        "\n",
        "# Predict:\n",
        "out = net(input)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28)\n",
            "target = 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 17.3649, -22.0449,   8.0199,   3.7973,   2.7276,   9.8694,   6.9859,\n",
            "          10.6175,   4.9958,   7.1057]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duQUCaXOo_zD"
      },
      "source": [
        "Now the highest output is assigned to the correct target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZya0QZ6q1-k"
      },
      "source": [
        "Accuracy on training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1WcjW5_pCqx",
        "outputId": "928b0bb0-580e-44e8-e8d2-bb5dd45dabf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input = torch.tensor(train_images, dtype=torch.float)\n",
        "out = net(input)\n",
        "_, predicted = torch.max(out, 1)\n",
        "(predicted.numpy() == train_labels).sum() / 60000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9403833333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9Ick4n0LiFM"
      },
      "source": [
        "Accuracy on test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ_ChRoBrAVp",
        "outputId": "7f9f0b78-6cff-4643-8dce-356e5dc639f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input = torch.tensor(test_images, dtype=torch.float)\n",
        "out = net(input)\n",
        "_, predicted = torch.max(out, 1)\n",
        "(predicted.numpy() == test_labels).sum() / 10000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.937"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s0RalLicamt"
      },
      "source": [
        "PyTorch offers some utilities to process batches of data:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYm-GgOpMegb"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfOh-OtOdY9M"
      },
      "source": [
        "Let us build ``DataLoader``s for training and test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruc-zjUsch3a"
      },
      "source": [
        "tensor_x = torch.tensor(train_images, dtype=torch.float)\n",
        "tensor_y = torch.tensor(train_labels, dtype=torch.long)\n",
        "train_dataset = TensorDataset(tensor_x, tensor_y) \n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1000, shuffle=True) \n",
        "\n",
        "tensor_x = torch.tensor(test_images, dtype=torch.float)\n",
        "tensor_y = torch.tensor(test_labels, dtype=torch.long)\n",
        "test_dataset = TensorDataset(tensor_x,tensor_y) \n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1000, shuffle=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YmbmnGLptjM"
      },
      "source": [
        "New network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXiHXKcSd3pG"
      },
      "source": [
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2CMHzKMd35s"
      },
      "source": [
        "Reimplement the training loop using epochs and batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR0cEnSWd3kf",
        "outputId": "f354d570-0147-41ff-9c85-e4a2f2e4894d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "nepochs = 10\n",
        "hh = []\n",
        "for it in range(nepochs): \n",
        "  rloss = 0.0\n",
        "  for x, t in train_dataloader:   \n",
        "    # Reseteo los gradientes a 0:\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass:\n",
        "    outputs = net(x)\n",
        "    loss = criterion(outputs, t)\n",
        "\n",
        "    # Backward pass:\n",
        "    loss.backward()\n",
        "\n",
        "    # Actualización de los pesos:\n",
        "    optimizer.step()\n",
        "\n",
        "    # Acumulo loss:\n",
        "    rloss += loss.item()\n",
        "\n",
        "  # Imprimir loss de la época:\n",
        "  print('[%d] loss: %.3f' % (it + 1, rloss))\n",
        "  hh.append(rloss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1] loss: 122.664\n",
            "[2] loss: 27.832\n",
            "[3] loss: 20.689\n",
            "[4] loss: 17.423\n",
            "[5] loss: 15.558\n",
            "[6] loss: 14.343\n",
            "[7] loss: 13.581\n",
            "[8] loss: 12.811\n",
            "[9] loss: 12.272\n",
            "[10] loss: 11.820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F96yqgS3gGZF"
      },
      "source": [
        "Accuracy on training and test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z2KFUhBqD1C",
        "outputId": "547158cd-5c7e-486c-ecff-5962ad7ae525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input = torch.tensor(train_images, dtype=torch.float)\n",
        "out = net(input)\n",
        "_, predicted = torch.max(out, 1)\n",
        "(predicted.numpy() == train_labels).sum() / 60000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9442833333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN3Bc0qtd3hB",
        "outputId": "7b5c5bb6-913e-4d6a-8209-425b26d64907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input = torch.tensor(test_images, dtype=torch.float)\n",
        "out = net(input)\n",
        "_, predicted = torch.max(out, 1)\n",
        "(predicted.numpy() == test_labels).sum() / 10000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9376"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNu0TkFdqM64"
      },
      "source": [
        "### <font color=\"#CA3532\">Exercise 3</font>\n",
        "\n",
        "Use PyTorch to implement a convolutional neural network and apply it to classify the images of the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. The network should have at least the following characteristics:\n",
        "\n",
        "- Convolutional layers.\n",
        "- Pooling layers.\n",
        "- Some regularization mechanism, such as dropout or L2 regularization.\n",
        "- Batch normalization layers.\n",
        "\n",
        "The network should obtain at least **75%** accuracy on the test set.\n",
        "\n",
        "The example in the [Training a classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) tutorial may be a good starting point.\n",
        "\n",
        "You can also check the available PyTorch layers in the [torch.nn](https://pytorch.org/docs/stable/nn.html) module documentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-VWDmzv_Uwy"
      },
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# TO-DO: Implement a CNN for the CIFAR-10 dataset. Use this cell and the\n",
        "# followings to include your code.\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2eRhcJfhn99"
      },
      "source": [
        "---\n",
        "# <font color=\"#CA3532\">Lab session 2</font>\n",
        "---\n",
        "\n",
        "Develop a cool project using some of the techniques and tools that you have learnt in this course. \n",
        "\n",
        "More details soon.\n",
        "\n",
        "Need inspiration? See the kind of things other students like you are doing [out there](http://cs231n.stanford.edu/2017/reports.html)."
      ]
    }
  ]
}